{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Autogen"
      ],
      "metadata": {
        "id": "1pu9EJTtNDdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Case\n",
        "\n",
        "- Firstly we will upload a pdf or document\n",
        "- next we will have some questions in normal text format\n",
        "- then the agents come into action, first agent is to accept the question and check whether it is relevant to the given document\n",
        "- then the retrieval agent and it will rank the answers accordingly (top 3)\n",
        "- next answer genertaion\n",
        "- next validation\n",
        "- if the answer is correct it will give it to human agent for the feedback and it will store in the vector db\n",
        "- if the answer is no, then it will start the process from retrieval again\n"
      ],
      "metadata": {
        "id": "axPt_p952XnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autogen PyPDF2 chromadb groq langchain-community huggingface pypdf"
      ],
      "metadata": {
        "id": "nHbAE3-eM99I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "import chromadb\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Load Llama model (Assuming Llama2 is hosted locally or via API)\n",
        "from transformers import pipeline\n",
        "config_list =[\n",
        "    {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"api_key\": \"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\",\n",
        "        \"api_type\":\"groq\",\n",
        "        \"max_completion_tokens\":2000,\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load embeddings model for semantic similarity\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=\"document_qa\")\n",
        "\n",
        "# Load and process document\n",
        "file_path = \"/content/10-Q_-_Tesla_INC_-_10-24-2024.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# Store document chunks in ChromaDB\n",
        "documents = []\n",
        "for idx, text in enumerate(texts):\n",
        "    documents.append(text.page_content)\n",
        "    chroma_collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[text.page_content],\n",
        "        metadatas=[{\"source\": f\"Page {text.metadata['page']}\"}]\n",
        "    )\n",
        "\n",
        "# Define AutoGen Agents\n",
        "#config_list = [{\"model\": \"gemma2-9b-it\"}]  # Placeholder, replace with actual model\n",
        "\n",
        "question_verification_agent = autogen.AssistantAgent(\n",
        "    name=\"QuestionVerificationAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Verify if the given question is relevant to the provided document.\")\n",
        "\n",
        "retrieval_agent = autogen.AssistantAgent(\n",
        "    name=\"RetrievalAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Retrieve the most relevant passages from the document.\")\n",
        "\n",
        "answer_generation_agent = autogen.AssistantAgent(\n",
        "    name=\"AnswerGenerationAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Generate a refined answer based on retrieved document chunks.\")\n",
        "\n",
        "validation_agent = autogen.AssistantAgent(\n",
        "    name=\"ValidationAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Validate the generated answer against the retrieved text.\")\n",
        "\n",
        "human_feedback_agent = autogen.AssistantAgent(\n",
        "    name=\"HumanFeedbackAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Store validated answers in ChromaDB.\")\n",
        "\n",
        "# Ask a question\n",
        "question = \"What are the key findings in the document?\"\n",
        "relevance_check = question_verification_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": question}])\n",
        "if \"not relevant\" in relevance_check:\n",
        "    print(\"The question is not relevant to the document.\")\n",
        "else:\n",
        "    retrieved_texts = retrieval_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Documents: {documents}\\n\\nQuestion: {question}\"}])\n",
        "    generated_answer = answer_generation_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"{retrieved_texts}\\n\\nQuestion: {question}\"}])\n",
        "    validation_result = validation_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Answer: {generated_answer}\\nRetrieved: {retrieved_texts}\"}])\n",
        "\n",
        "    if \"valid\" in validation_result:\n",
        "        human_feedback_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Question: {question}\\nAnswer: {generated_answer}\"}])\n",
        "        print(\"Answer:\", generated_answer)\n",
        "    else:\n",
        "        print(\"Answer validation failed. Restarting retrieval...\")\n"
      ],
      "metadata": {
        "id": "pXB1mCFeVgzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Autogen version1 without human feedback"
      ],
      "metadata": {
        "id": "abQsc2hy6AAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "import chromadb\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Load Llama model (Assuming Llama2 is hosted locally or via API)\n",
        "from transformers import pipeline\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"api_key\": \"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\",\n",
        "        \"api_type\": \"groq\",\n",
        "        \"max_completion_tokens\": 2000,\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load embeddings model for semantic similarity\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=\"document_qa\")\n",
        "\n",
        "# Load and process document\n",
        "file_path = \"/content/10-Q_-_Tesla_INC_-_10-24-2024.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# Store document chunks in ChromaDB\n",
        "documents = []\n",
        "for idx, text in enumerate(texts):\n",
        "    documents.append(text.page_content)\n",
        "    chroma_collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[text.page_content],\n",
        "        metadatas=[{\"source\": f\"Page {text.metadata['page']}\"}]\n",
        "    )\n",
        "\n",
        "# Function to retrieve top 3 relevant chunks\n",
        "def retrieve_relevant_chunks(question):\n",
        "    query_embedding = embedding_model.encode(question, convert_to_tensor=True)\n",
        "\n",
        "    retrieved_chunks = []\n",
        "    for doc in documents:\n",
        "        doc_embedding = embedding_model.encode(doc, convert_to_tensor=True)\n",
        "        similarity = util.pytorch_cos_sim(query_embedding, doc_embedding)\n",
        "        retrieved_chunks.append((doc, similarity.item()))\n",
        "\n",
        "    # Sort by similarity score and take top 3\n",
        "    retrieved_chunks = sorted(retrieved_chunks, key=lambda x: x[1], reverse=True)[:3]\n",
        "    return \"\\n\\n\".join([chunk[0] for chunk in retrieved_chunks])\n",
        "\n",
        "# Define AutoGen Agents\n",
        "question_verification_agent = autogen.AssistantAgent(\n",
        "    name=\"QuestionVerificationAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Verify if the given question is relevant to the provided document.\"\n",
        ")\n",
        "\n",
        "retrieval_agent = autogen.AssistantAgent(\n",
        "    name=\"RetrievalAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Retrieve the most relevant passages from the document.\"\n",
        ")\n",
        "\n",
        "answer_generation_agent = autogen.AssistantAgent(\n",
        "    name=\"AnswerGenerationAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Generate a refined answer based on retrieved document chunks.\"\n",
        ")\n",
        "\n",
        "validation_agent = autogen.AssistantAgent(\n",
        "    name=\"ValidationAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Validate the generated answer against the retrieved text.\"\n",
        ")\n",
        "\n",
        "human_feedback_agent = autogen.AssistantAgent(\n",
        "    name=\"HumanFeedbackAgent\",\n",
        "    llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Store validated answers in ChromaDB.\"\n",
        ")\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the future growth of tesla? what are the accounting qualities in tesla? tell me the growth in the last 4 months? what are the different revenue segments in tesla\"\n",
        "relevance_check = question_verification_agent.generate_reply(\n",
        "    messages=[{\"role\": \"user\", \"content\": f\"Document:\\n{documents[:5]}\\n\\nQuestion: {question}\"}]  # Passing first 5 chunks\n",
        ")\n",
        "\n",
        "if \"not relevant\" in relevance_check:\n",
        "    print(\"The question is not relevant to the document.\")\n",
        "else:\n",
        "    retrieved_texts = retrieve_relevant_chunks(question)  # Get only top 3 relevant chunks\n",
        "    retrieved_reply = retrieval_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Relevant Documents: {retrieved_texts}\\n\\nQuestion: {question}\"}])\n",
        "    generated_answer = answer_generation_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"{retrieved_reply}\\n\\nQuestion: {question}\"}])\n",
        "    validation_result = validation_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Answer: {generated_answer}\\nRetrieved: {retrieved_reply}\"}])\n",
        "\n",
        "    if \"valid\" in validation_result:\n",
        "        human_feedback_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Question: {question}\\nAnswer: {generated_answer}\"}])\n",
        "        print(\"Answer:\", generated_answer)"
      ],
      "metadata": {
        "id": "whQZM31Uu_rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevance_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFuiynSUwiwS",
        "outputId": "25a6852b-1592-45e5-f3c2-da7b50c1a8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': \"The provided document is a Form 10-Q filed by Tesla, Inc. with the US Securities and Exchange Commission (SEC) for the quarter ended September 30, 2024. This document primarily focuses on Tesla's financial statements, management's discussion of financial condition and results of operations, risk factors, and other relevant information. However, it doesn't provide specific information regarding the future growth of Tesla, accounting qualities, the growth in the last 4 months, or different revenue segments.\\n\\nTo answer these questions, one would need to refer to additional sources that provide Tesla's strategic plans, financial performance analysis, and segment-wise revenue information. It is also important to note that the company's growth prospects, accounting qualities, and past performance can be assessed by analyzing historical financial reports, industry trends, and expert opinions.\",\n",
              " 'refusal': None,\n",
              " 'role': 'assistant',\n",
              " 'audio': None,\n",
              " 'function_call': None,\n",
              " 'tool_calls': None}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC6svKDT0OY4",
        "outputId": "1a947907-d1d5-4e7f-97a0-6cbcc0a23a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': \"The generated answer is valid as it accurately summarizes the information provided in the retrieved text. Both the generated answer and the retrieved text discuss Tesla's strong growth in total revenues, the preparation and auditing of Tesla's financial statements, and the company's three main revenue segments.\\n\\nHowever, there are a few minor differences between the two texts. The generated answer states that total revenues increased from $23,350 million in Q3 2023 to $71,983 million in Q3 2024 when comparing the nine-month periods. However, the retrieved text correctly states that the increase is from Q3 2023 to the same period in 2024. The generated answer also mentions that revenues increased in the last 4 months, while the retrieved text states that the information is not provided. These differences do not affect the validity of the generated answer, as the main points and information are accurately presented.\",\n",
              " 'refusal': None,\n",
              " 'role': 'assistant',\n",
              " 'audio': None,\n",
              " 'function_call': None,\n",
              " 'tool_calls': None}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I1wDuoWyksR",
        "outputId": "69df25be-089d-4540-a7f3-43176aa95736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': \"The future growth of Tesla has seen an increase in total revenues from $23,350 million in the nine months ended September 30, 2023, to $71,983 million in the same period of 2024, indicating strong growth.\\n\\nTesla's financial statements are prepared in accordance with generally accepted accounting principles (GAAP), audited by an independent auditing firm, and present financial information in a clear and transparent manner with notes providing additional details and explanations.\\n\\nIn the last 4 months, Tesla's total revenues have increased from $57,879 million in the three months ended June 30, 2024, to $71,983 million in the nine months ended September 30, 2024, indicating strong growth over this period.\\n\\nTesla has three main revenue segments:\\n1. Automotive: This segment includes the sale of electric vehicles and related regulatory credits, as well as leasing and financing services.\\n2. Energy generation and storage: This segment includes the sale of solar energy systems, energy storage products, and related services.\\n3. Services and other: This segment includes revenues from services such as vehicle maintenance and repair, as well as sales of merchandise and other products.\",\n",
              " 'refusal': None,\n",
              " 'role': 'assistant',\n",
              " 'audio': None,\n",
              " 'function_call': None,\n",
              " 'tool_calls': None}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Autogen version3 without using groupchat"
      ],
      "metadata": {
        "id": "XTX33Z3wrC2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "import chromadb\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Llama model (Assuming Llama2 is hosted locally or via API)\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"api_key\": \"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\",\n",
        "        \"api_type\": \"groq\",\n",
        "        \"max_completion_tokens\": 3500,\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load embeddings model for semantic similarity\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=\"document_qa\")\n",
        "\n",
        "# Load and process document\n",
        "file_path = \"10-Q_-_Tesla_INC_-_10-24-2024.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# Store document chunks in ChromaDB\n",
        "documents = []\n",
        "for idx, text in enumerate(texts):\n",
        "    documents.append(text.page_content)\n",
        "    chroma_collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[text.page_content],\n",
        "        metadatas=[{\"source\": f\"Page {text.metadata['page']}\"}]\n",
        "    )\n",
        "\n",
        "# Function to retrieve top 3 relevant chunks\n",
        "def retrieve_relevant_chunks(question):\n",
        "    query_embedding = embedding_model.encode(question, convert_to_tensor=True)\n",
        "    retrieved_chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        doc_embedding = embedding_model.encode(doc, convert_to_tensor=True)\n",
        "        similarity = util.pytorch_cos_sim(query_embedding, doc_embedding)\n",
        "        retrieved_chunks.append((doc, similarity.item()))\n",
        "\n",
        "    retrieved_chunks = sorted(retrieved_chunks, key=lambda x: x[1], reverse=True)[:3]\n",
        "    return \"\\n\\n\".join([chunk[0] for chunk in retrieved_chunks])\n",
        "\n",
        "question_verification_agent = autogen.AssistantAgent(name=\"QuestionVerificationAgent\",llm_config={\"config_list\": config_list},\n",
        "                                                     system_message=\"Verify if the given question is relevant to the provided document.\")\n",
        "\n",
        "retrieval_agent = autogen.AssistantAgent(name=\"RetrievalAgent\",llm_config={\"config_list\": config_list},\n",
        "                                         system_message=\"Retrieve the most relevant passages from the document.\")\n",
        "\n",
        "answer_generation_agent = autogen.AssistantAgent(name=\"AnswerGenerationAgent\",llm_config={\"config_list\": config_list},\n",
        "                                                 system_message=\"Generate a refined answer based on retrieved document chunks.\")\n",
        "\n",
        "validation_agent = autogen.AssistantAgent(name=\"ValidationAgent\",llm_config={\"config_list\": config_list},\n",
        "                                          system_message=\"Validate the generated answer against the retrieved text.\")\n",
        "\n",
        "human_feedback_agent = autogen.UserProxyAgent(name=\"HumanFeedbackAgent\",system_message=\"Provide human feedback on the generated answer. Reply 'Yes' if correct, 'No' if incorrect.\")\n",
        "\n",
        "\n",
        "def ask_question(question):\n",
        "    # Step 1: Question relevance check\n",
        "    relevance_check = question_verification_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Document:\\n{documents[:5]}\\n\\nQuestion: {question}\"}])\n",
        "\n",
        "    if \"not relevant\" in relevance_check.get(\"content\", \"\").lower():\n",
        "        print(\"The question is not relevant to the document.\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        # Step 2: Retrieve relevant document chunks\n",
        "        retrieved_texts = retrieve_relevant_chunks(question)\n",
        "        retrieved_reply = retrieval_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Relevant Documents: {retrieved_texts}\\n\\nQuestion: {question}\"}])\n",
        "        generated_answer = answer_generation_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"{retrieved_reply}\\n\\nQuestion: {question}\"}])\n",
        "        validation_result = validation_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": f\"Answer: {generated_answer}\\nRetrieved: {retrieved_reply}\"}])\n",
        "\n",
        "        # Step 3: Human Feedback\n",
        "        while True:\n",
        "            print(\"\\n Generated Answer:\")\n",
        "            print(generated_answer.get(\"content\", \"\"))\n",
        "\n",
        "            user_feedback = input(\"\\n Is this answer correct? (yes/no): \").strip().lower()\n",
        "\n",
        "            if user_feedback == \"yes\":\n",
        "                chroma_collection.add(\n",
        "                    ids=[str(len(documents) + 1)],\n",
        "                    documents=[generated_answer.get(\"content\", \"\")],  # Extract string\n",
        "                )\n",
        "                print(\" ✅ Answer stored in ChromaDB.\")\n",
        "                return\n",
        "            elif user_feedback == \"no\":\n",
        "                print(\" Feedback rejected. Re-attempting retrieval...\")\n",
        "                break\n",
        "            else:\n",
        "                print(\" Invalid input. Please type 'yes' or 'no'.\")\n",
        "\n",
        "# Example question\n",
        "question = \"What is the future growth of Tesla? What are the accounting qualities in Tesla? Tell me the growth in the last 4 months? What are the different revenue segments in Tesla?\"\n",
        "ask_question(question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp8Wk4veHefc",
        "outputId": "b47f9875-e217-4915-8308-055192af2978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 0\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 0\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 1\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 1\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 2\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 2\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 3\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 3\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 4\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 4\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 5\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 5\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 6\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 6\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 7\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 7\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 8\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 8\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 9\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 9\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 10\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 10\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 11\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 11\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 12\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 12\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 13\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 13\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 14\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 14\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 15\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 15\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 16\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 16\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 17\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 17\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 18\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 18\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 19\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 19\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 20\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 20\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 21\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 21\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 22\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 22\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 23\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 23\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 24\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 24\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 25\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 25\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 26\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 26\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 27\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 27\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 28\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 28\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 29\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 29\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 30\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 30\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 31\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 31\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 32\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 32\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 33\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 33\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 34\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 34\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 35\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 35\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 36\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 36\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 37\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 37\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 38\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 38\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 39\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 39\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 40\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 40\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 41\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 41\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 42\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 42\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 43\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 43\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 44\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 44\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 45\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 45\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 46\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 46\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 47\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 47\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 48\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 48\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 49\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 49\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 50\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 50\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 51\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 51\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 52\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 52\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 53\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 53\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 54\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 54\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 55\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 55\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 56\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 56\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 57\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 57\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 58\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 58\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 59\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 59\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 60\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 60\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 61\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 61\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 62\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 62\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 63\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 63\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 64\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 64\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 65\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 65\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 66\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 66\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 67\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 67\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 68\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 68\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 69\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 69\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 70\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 70\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 71\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 71\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 72\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 72\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 73\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 73\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 74\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 74\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 75\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 75\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 76\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 76\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 77\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 77\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 78\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 78\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 79\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 79\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 80\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 80\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 81\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generated Answer:\n",
            "The future growth of Tesla, as of September 30, 2024, includes total revenues of $71,983 million for the nine months ended, an increase of 16.7% compared to the same period in 2023. The net income attributable to common stockholders for the same period was $4,774 million, an increase of 166.1% compared to 2023.\n",
            "\n",
            "In terms of accounting qualities, Tesla's consolidated financial statements are prepared in accordance with U.S. GAAP. The company has a history of generating positive net income and cash flows from operations. However, Tesla has a significant amount of intangible assets, such as goodwill and trademarks, which are subject to impairment.\n",
            "\n",
            "In the last 4 months, specifically the third quarter of 2024, Tesla's total revenues were $25,182 million, an increase of 7.8% compared to the same period in 2023. The net income attributable to common stockholders for this quarter was $2,167 million, an increase of 15.7% compared to 2023.\n",
            "\n",
            "The different revenue segments in Tesla include:\n",
            "1. Automotive revenues: This segment includes sales of automotive vehicles, regulatory credits, and leasing.\n",
            "2. Energy generation and storage: This segment includes sales of energy generation and storage products, such as solar panels and powerwall batteries.\n",
            "3. Services and other: This segment includes services related to its products, such as maintenance and repair services, as well as sales of merchandise and other related revenues.\n",
            "\n",
            " Is this answer correct? (yes/no): yes\n",
            " ✅ Answer stored in ChromaDB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autogen using round robin groupchat version 4 and main version"
      ],
      "metadata": {
        "id": "OCsJf5kfkfuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import autogen\n",
        "import chromadb\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Groq API key from environment variable\n",
        "api_key = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"mixtral-8x7b-32768\",\n",
        "        \"api_key\": api_key,\n",
        "        \"api_type\": \"groq\",\n",
        "        \"max_completion_tokens\": 3500,\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load embeddings model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB (fresh start)\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_client.delete_collection(name=\"document_qa\")  # Reset to avoid old data\n",
        "chroma_collection = chroma_client.create_collection(name=\"document_qa\")\n",
        "\n",
        "# Load and process document\n",
        "file_path = \"10-Q_-_Tesla_INC_-_10-24-2024.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} pages from {file_path}\")\n",
        "print(\"Sample content:\", docs[0].page_content[:200])\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# Store document chunks in ChromaDB\n",
        "documents = []\n",
        "for idx, text in enumerate(texts):\n",
        "    documents.append(text.page_content)\n",
        "    chroma_collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[text.page_content],\n",
        "        metadatas=[{\"source\": f\"Page {text.metadata['page']}\"}]\n",
        "    )\n",
        "\n",
        "# Function to retrieve top 3 relevant chunks from ChromaDB\n",
        "def retrieve_relevant_chunks(question):\n",
        "    query_embedding = embedding_model.encode(question, convert_to_tensor=False).tolist()\n",
        "    results = chroma_collection.query(query_embeddings=[query_embedding], n_results=3)\n",
        "    retrieved_texts = results[\"documents\"][0] if \"documents\" in results and results[\"documents\"] else []\n",
        "    return \"\\n\\n\".join(retrieved_texts) if retrieved_texts else \"No relevant chunks found.\"\n",
        "\n",
        "# Define AutoGen agents\n",
        "question_verification_agent = autogen.AssistantAgent(\n",
        "    name=\"QuestionVerificationAgent\",llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Verify if the given question is relevant to the provided document.\"\n",
        ")\n",
        "\n",
        "retrieval_agent = autogen.AssistantAgent(\n",
        "    name=\"RetrievalAgent\",llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Retrieve the most relevant passages from the document.\"\n",
        ")\n",
        "\n",
        "answer_generation_agent = autogen.AssistantAgent(\n",
        "    name=\"AnswerGenerationAgent\",llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Generate a refined answer based on retrieved document chunks.\"\n",
        ")\n",
        "\n",
        "validation_agent = autogen.AssistantAgent(\n",
        "    name=\"ValidationAgent\",llm_config={\"config_list\": config_list},\n",
        "    system_message=\"Validate the generated answer against the retrieved text.\"\n",
        ")\n",
        "\n",
        "human_feedback_agent = autogen.UserProxyAgent(\n",
        "    name=\"HumanFeedbackAgent\",\n",
        "    system_message=\"Provide human feedback on the generated answer. Reply 'Yes' if correct, 'No' if incorrect.\"\n",
        ")\n",
        "\n",
        "# Define GroupChat and Manager\n",
        "group_chat = autogen.GroupChat(\n",
        "    agents=[question_verification_agent, retrieval_agent, answer_generation_agent, validation_agent, human_feedback_agent],messages=[],\n",
        "    speaker_selection_method=\"round_robin\"\n",
        ")\n",
        "\n",
        "manager = autogen.GroupChatManager(\n",
        "    groupchat=group_chat,\n",
        "    llm_config={\"config_list\": config_list}\n",
        ")\n",
        "\n",
        "# Function to ask a question\n",
        "def ask_question(question, retries=3):\n",
        "    if retries <= 0:\n",
        "        print(\"Max retries reached. Please refine your question.\")\n",
        "        return\n",
        "\n",
        "    retrieved_texts = retrieve_relevant_chunks(question)\n",
        "    if \"No relevant chunks\" in retrieved_texts:\n",
        "        print(retrieved_texts)\n",
        "        return\n",
        "\n",
        "    initial_message = f\"Relevant Documents:\\n{retrieved_texts}\\n\\nQuestion: {question}\"\n",
        "    print(f\"\\nInitiating chat with question: {question}\")\n",
        "\n",
        "    chat_result = manager.initiate_chat(\n",
        "        recipient=question_verification_agent,\n",
        "        message=initial_message,\n",
        "        max_turns=5,\n",
        "    )\n",
        "\n",
        "    generated_answer = \"\"\n",
        "    for msg in chat_result.chat_history:\n",
        "        if msg[\"name\"] == \"AnswerGenerationAgent\":\n",
        "            generated_answer = msg[\"content\"]\n",
        "            break\n",
        "\n",
        "    if not generated_answer:\n",
        "        print(\"No answer generated by AnswerGenerationAgent.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    print(generated_answer)\n",
        "\n",
        "    user_feedback = input(\"\\nIs this answer correct? (yes/no): \").strip().lower()\n",
        "    if user_feedback == \"yes\":\n",
        "        new_id = str(len(documents) + 1)\n",
        "        chroma_collection.add(\n",
        "            ids=[new_id],\n",
        "            documents=[generated_answer]\n",
        "        )\n",
        "        print(f\"✅ Answer stored in ChromaDB with ID {new_id}.\")\n",
        "    elif user_feedback == \"no\":\n",
        "        print(\"Feedback rejected. Re-attempting retrieval...\")\n",
        "        ask_question(question, retries - 1)\n",
        "\n",
        "# Example question\n",
        "question = \"What are the Material Cash Requirements in Tesla according to the 10-Q?\"\n",
        "ask_question(question)"
      ],
      "metadata": {
        "id": "uNRMJ2J8CYyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##crew-ai (didnt work, asking for openai key)"
      ],
      "metadata": {
        "id": "3-tGfmpG58RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install crewai"
      ],
      "metadata": {
        "id": "ueCDWN-EcaqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from crewai import Task, Agent, Crew\n",
        "from transformers import pipeline\n",
        "\n",
        "llm =[\n",
        "    {\n",
        "        \"model\": \"gemma2-9b-it\",\n",
        "        \"api_key\": \"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\",\n",
        "        \"api_type\":\"groq\",\n",
        "        \"max_completion_tokens\":2000,\n",
        "    }\n",
        "]\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(name=\"document_qa\")\n",
        "\n",
        "loader = PyPDFLoader(\"/content/10-Q_-_Tesla_INC_-_10-24-2024.pdf\")\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "for idx, text in enumerate(texts):\n",
        "    chroma_collection.add(\n",
        "        ids=[str(idx)],\n",
        "        documents=[text.page_content],\n",
        "        metadatas=[{\"source\": f\"Page {text.metadata['page']}\"}]\n",
        "    )\n",
        "\n",
        "question_verification_agent = Agent(\n",
        "    name=\"Question Verification Agent\",\n",
        "    role=\"Verifier\",\n",
        "    goal=\"Verify if the given question is relevant to the provided document.\",\n",
        "    backstory=\"An AI-powered verifier trained to assess whether a question aligns with the document content, ensuring relevant queries are processed further.\"\n",
        ")\n",
        "\n",
        "retrieval_agent = Agent(\n",
        "    name=\"Retrieval Agent\",\n",
        "    role=\"Retriever\",\n",
        "    goal=\"Retrieve the most relevant passages from the document. Return only the top 3 ranked chunks.\",\n",
        "    backstory=\"A highly efficient AI-powered retriever designed to fetch the most relevant document sections using advanced semantic search techniques.\"\n",
        ")\n",
        "\n",
        "answer_generation_agent = Agent(\n",
        "    name=\"Answer Generation Agent\",\n",
        "    role=\"Generator\",\n",
        "    goal=\"Generate a refined answer based on the retrieved document chunks. Keep responses concise.\",\n",
        "    backstory=\"An advanced language model-based agent that synthesizes information from retrieved text and generates well-structured, accurate answers.\"\n",
        ")\n",
        "\n",
        "validation_agent = Agent(\n",
        "    name=\"Validation Agent\",\n",
        "    role=\"Validator\",\n",
        "    goal=\"Validate the generated answer against the retrieved text. If invalid, request another retrieval cycle.\",\n",
        "    backstory=\"A strict AI reviewer ensuring the generated answers align with the retrieved content, preventing misinformation.\"\n",
        ")\n",
        "\n",
        "human_feedback_agent = Agent(\n",
        "    name=\"Human Feedback Agent\",\n",
        "    role=\"Reviewer\",\n",
        "    goal=\"Store validated answers in ChromaDB.\",\n",
        "    backstory=\"A human-in-the-loop AI responsible for collecting and incorporating expert feedback into the database for continuous improvement.\"\n",
        ")\n",
        "\n",
        "question_verification_task = Task(\n",
        "    description=\"Check if the question is relevant to the document.\",\n",
        "    agent=question_verification_agent,\n",
        "    expected_output=\"A response indicating whether the question is relevant or not.\"\n",
        ")\n",
        "\n",
        "retrieval_task = Task(\n",
        "    description=\"Retrieve top 3 relevant document chunks.\",\n",
        "    agent=retrieval_agent,\n",
        "    expected_output=\"A list of the top 3 relevant document chunks from the database.\"\n",
        ")\n",
        "\n",
        "answer_generation_task = Task(\n",
        "    description=\"Generate a concise answer using retrieved chunks.\",\n",
        "    agent=answer_generation_agent,\n",
        "    expected_output=\"A well-structured answer generated from the retrieved text.\"\n",
        ")\n",
        "\n",
        "validation_task = Task(\n",
        "    description=\"Validate the answer against retrieved text.\",\n",
        "    agent=validation_agent,\n",
        "    expected_output=\"A confirmation if the generated answer is valid or a request to redo retrieval.\"\n",
        ")\n",
        "\n",
        "feedback_task = Task(\n",
        "    description=\"Store validated answers in ChromaDB.\",\n",
        "    agent=human_feedback_agent,\n",
        "    expected_output=\"The validated answer is stored in ChromaDB with feedback.\"\n",
        ")\n",
        "\n",
        "# Create CrewAI workflow\n",
        "crew = Crew(\n",
        "    agents=[\n",
        "        question_verification_agent, retrieval_agent, answer_generation_agent, validation_agent, human_feedback_agent\n",
        "    ],\n",
        "    tasks=[question_verification_task, retrieval_task, answer_generation_task, validation_task, feedback_task],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Main Process Execution\n",
        "question = \"What are the key findings in the document?\"\n",
        "verification_result = crew.kickoff(inputs={\"question\": question})\n",
        "\n",
        "if \"not relevant\" in verification_result:\n",
        "    print(\"The question is not relevant to the document.\")\n",
        "else:\n",
        "    retrieved_texts = crew.kickoff(inputs={\"question\": question})\n",
        "    generated_answer = crew.kickoff(inputs={\"retrieved_texts\": retrieved_texts})\n",
        "    validation_result = crew.kickoff(inputs={\"answer\": generated_answer})\n",
        "\n",
        "    if \"valid\" in validation_result:\n",
        "        crew.kickoff(inputs={\"answer\": generated_answer}, tasks=[feedback_task])\n",
        "        print(\"Answer:\", generated_answer)\n",
        "    else:\n",
        "        print(\"Answer validation failed. Reattempting retrieval.\")\n"
      ],
      "metadata": {
        "id": "37_rflK0cc66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}